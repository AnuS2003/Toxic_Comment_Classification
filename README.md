# Toxic_Comment_Classification
This project aims to classify comments into various toxicity categories using advanced machine learning models. We compare the performance of BERT (Bidirectional Encoder Representations from Transformers) and LSTM (Long Short-Term Memory) models to determine the best approach for detecting toxic content in text.

# Models
<strong>BERT: </strong>Utilizes bidirectional attention to capture nuanced contextual information for precise classification of toxic comments.<br />
<strong>LSTM: </strong>Processes text sequentially to understand context over time, aiming to predict comment toxicity effectively.

<h1>Streamlit Application</h1>
The project includes a user-friendly Streamlit application that provides:

Real-Time Toxicity Detection -> Input a comment and receive immediate feedback on its toxicity level.<br />
Interactive Interface -> A clean and intuitive UI to interact with the model.<br />
Real-time Feedback -> Displays detailed feedback for various toxicity categories.

# Results
The BERT model has shown superior performance with higher accuracy and precision in distinguishing between toxic and non-toxic comments, while LSTM may exhibit limitations in capturing complex textual nuances.


